{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e0d17c97-1480-44f2-9957-745f8f11c1c4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Run initial set up code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "503a5dd2-7ff0-44f3-888a-66061124f21e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run /Workspace/Users/mbothe7@hotmail.com/databricks-end-to-end-project/01-initial-config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "56ae9843-cfe7-4c5d-b0b1-004e00e282da",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Setting up initial objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d2980e63-bea0-4d82-8aae-b03993a3f83e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class InitialSetupHelper():\n",
    "    def __init__(self,env):\n",
    "        Conf = InitialConfig()\n",
    "        self.landing_zone = Conf.landing_path+\"/raw\"\n",
    "        self.checkpoint_path   = Conf.checkpoint_path\n",
    "        self.root_path = Conf.root_path\n",
    "        self.silver_path = Conf.silver_path\n",
    "        self.bronze_path = Conf.bronze_path\n",
    "        self.good_path = Conf.gold_path\n",
    "        self.catalog      = env\n",
    "        #self.db_name      = Conf.db_name\n",
    "        self.initialized  = False\n",
    "\n",
    "    def create_catalog(self):\n",
    "        print(f\"Creating catalog {self.catalog} .....\")\n",
    "        spark.sql(f\"\"\"\n",
    "            CREATE CATALOG IF NOT EXISTS {self.catalog} \n",
    "            MANAGED LOCATION '{self.root_path}root'\n",
    "        \"\"\")\n",
    "        print(f\"Catalog {self.catalog} created.\")\n",
    "  \n",
    "    def create_bronze_Schema(self):\n",
    "        print(f'Using {self.catalog} Catalog ')\n",
    "        spark.sql(f\"\"\" USE CATALOG {self.catalog}\"\"\")\n",
    "        print(f'Creating Bronze Schema in {self.catalog}')\n",
    "        spark.sql(f\"\"\"CREATE SCHEMA IF NOT EXISTS `bronze` MANAGED LOCATION '{self.bronze_path}'\"\"\")\n",
    "        print(\"Bronze Schema created.\")\n",
    "\n",
    "    \n",
    "    def create_silver_Schema(self):\n",
    "        print(f'Using {self.catalog} Catalog ')\n",
    "        spark.sql(f\"\"\" USE CATALOG '{self.catalog}'\"\"\")\n",
    "        print(f'Creating silver Schema in {self.catalog}')\n",
    "        spark.sql(f\"\"\"CREATE SCHEMA IF NOT EXISTS `silver` MANAGED LOCATION '{self.silver_path}'\"\"\") \n",
    "        print(\"Bronze Schema created.\")\n",
    "    \n",
    "    def create_gold_Schema(self):\n",
    "        print(f'Using {self.catalog} Catalog ')\n",
    "        spark.sql(f\"\"\" USE CATALOG `{self.catalog}`\"\"\")\n",
    "        print(f'Creating silver Schema in {self.catalog}')\n",
    "        spark.sql(f\"\"\"CREATE SCHEMA IF NOT EXISTS `gold` MANAGED LOCATION '{self.good_path}'\"\"\") \n",
    "        print(\"Bronze Schema created.\")\n",
    "\n",
    "    def create_roads_table(self):\n",
    "        print(f'Using {self.catalog} Catalog ')\n",
    "                \n",
    "        if (self.catalog):\n",
    "            print(f\"Creating roads table {self.catalog}.bronze.raw_roads .....\")\n",
    "            spark.sql(f\"\"\"CREATE TABLE IF NOT EXISTS {self.catalog}.bronze.raw_roads\n",
    "                    (\n",
    "                        Road_ID INT,\n",
    "                        Road_Category_Id INT,\n",
    "                        Road_Category VARCHAR(255),\n",
    "                        Region_ID INT,\n",
    "                        Region_Name VARCHAR(255),\n",
    "                        Total_Link_Length_Km DOUBLE,\n",
    "                        Total_Link_Length_Miles DOUBLE,\n",
    "                        All_Motor_Vehicles DOUBLE                      \n",
    "                        \n",
    "                    );\"\"\"\n",
    "                )\n",
    "            print(f\"Table  {self.catalog}.bronze.raw_roads created.\")         \n",
    "        else:\n",
    "            raise ReferenceError(\"Application database is not defined. Cannot create table in default database.\")\n",
    "\n",
    "    def create_raw_traffic_table(self):\n",
    "        spark.sql(f\"\"\" USE CATALOG '{self.catalog}'\"\"\")\n",
    "        if (self.catalog):\n",
    "            print(f\"Creating raw_traffic table {self.catalog}.bronze.raw_traffic .....\")\n",
    "            spark.sql(f\"\"\"CREATE TABLE IF NOT EXISTS `{self.catalog}`.`bronze`.`raw_traffic`\n",
    "                        (\n",
    "                            Record_ID INT,\n",
    "                            Count_point_id INT,\n",
    "                            Direction_of_travel VARCHAR(255),\n",
    "                            Year INT,\n",
    "                            Count_date VARCHAR(255),\n",
    "                            hour INT,\n",
    "                            Region_id INT,\n",
    "                            Region_name VARCHAR(255),\n",
    "                            Local_authority_name VARCHAR(255),\n",
    "                            Road_name VARCHAR(255),\n",
    "                            Road_Category_ID INT,\n",
    "                            Start_junction_road_name VARCHAR(255),\n",
    "                            End_junction_road_name VARCHAR(255),\n",
    "                            Latitude DOUBLE,\n",
    "                            Longitude DOUBLE,\n",
    "                            Link_length_km DOUBLE,\n",
    "                            Pedal_cycles INT,\n",
    "                            Two_wheeled_motor_vehicles INT,\n",
    "                            Cars_and_taxis INT,\n",
    "                            Buses_and_coaches INT,\n",
    "                            LGV_Type INT,\n",
    "                            HGV_Type INT,\n",
    "                            EV_Car INT,\n",
    "                            EV_Bike INT,\n",
    "                            create_at TIMESTAMP\n",
    "                    );\"\"\"\n",
    "                )\n",
    "            print(f\"Table  {self.catalog}.bronze.raw_traffic created.\")\n",
    "        else:\n",
    "            raise ReferenceError(\"Application database is not defined. Cannot create table in default database.\")\n",
    "\n",
    "    def initial_setup(self):\n",
    "        import time \n",
    "        print(f\"Initializing the environment. Please wait...\")\n",
    "        self.create_catalog()\n",
    "        self.create_bronze_Schema()\n",
    "        self.create_silver_Schema()\n",
    "        self.create_gold_Schema()\n",
    "        self.create_roads_table()\n",
    "        self.create_raw_traffic_table()\n",
    "        print(f\"Environment initialized.\")\n",
    "\n",
    "    def assert_table(self, table_name, schema):\n",
    "        print(f\"SHOW TABLES IN {self.catalog}.{schema}\")\n",
    "        spark.sql(f\"SHOW TABLES IN {self.catalog}.{schema}\")\n",
    "        assert spark.sql(f\"SHOW TABLES IN {self.catalog}.{schema}\") \\\n",
    "            .filter(f\"tableName=='{table_name}'\") \\\n",
    "            .count() == 1, f\"Table {table_name} not found\"\n",
    "        print(f\"Found {table_name} table in {self.catalog}.{schema} : Success\")\n",
    "\n",
    "\n",
    "\n",
    "    def validate_environment(self):\n",
    "        import time\n",
    "        start = int(time.time())\n",
    "        print(f\"Validating the environment. Please wait...\")\n",
    "        assert spark.sql(f\"SHOW DATABASES IN {self.catalog} \").filter(\"databaseName=='bronze'\").count() == 1, \"Bronze Schema not found\"\n",
    "        print(f\"Catalog {self.catalog}.bronze validated.\")\n",
    "\n",
    "        assert spark.sql(f\"SHOW DATABASES IN {self.catalog} \").filter(\"databaseName=='silver'\").count() == 1, \"Silver Schema not found\"\n",
    "        print(f\"Catalog {self.catalog}.'silver' validated.\")\n",
    "\n",
    "        assert spark.sql(f\"SHOW DATABASES IN {self.catalog} \").filter(\"databaseName=='gold'\").count() == 1, \"Gold Schema not found\"\n",
    "        print(f\"Catalog {self.catalog}.'gold' validated.\")\n",
    "\n",
    "        self.assert_table(\"raw_roads\",\"bronze\")\n",
    "        self.assert_table(\"raw_traffic\",\"bronze\")\n",
    "\n",
    "        print(f\"Environment validated in {int(time.time())-start} seconds.\")\n",
    "\n",
    "\n",
    "    def environment_cleanup(self):\n",
    "        print(f'Cleaning up the environment: {self.catalog}')\n",
    "        if spark.sql(f\"SHOW CATALOGS\").filter(f\"catalog == '{self.catalog}'\").count() == 1:\n",
    "            print(f\"Dropping the database {self.catalog}...\", end='')\n",
    "            spark.sql(f\"DROP CATALOG IF EXISTS {self.catalog} CASCADE\")\n",
    "            print(\"Done\")\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0e5dc901-3f64-428f-9bf6-1f1567bc55b8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#spark.sql(\"SHOW CATALOGS\").show()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 754521080215842,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "02-initial-setup",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
