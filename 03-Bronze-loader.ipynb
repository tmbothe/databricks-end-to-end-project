{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6e2b7d9d-2e62-4020-99d2-b9b5817ee5d3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Loading historical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1b5883cf-7d87-4b00-acba-1d9bff70e18d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#%run /Workspace/Users/mbothe7@hotmail.com/databricks-end-to-end-project/01-initial-config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "51ebf07b-5231-4bc1-a3f1-1cd3182cd12d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class BronzeLoader():\n",
    "    def __init__(self, env):\n",
    "        self.catalog = env\n",
    "        Conf = InitialConfig()\n",
    "        self.landing_path=Conf.landing_path\n",
    "        self.checkpoint_path= Conf.checkpoint_path+f'/{self.catalog}'\n",
    "        self.schema='bronze'\n",
    "        print(f'Checkpoint path is {self.checkpoint_path}')\n",
    "        print(f'Landing path is {self.landing_path}')\n",
    "        print(f'current catalog is {self.catalog}')\n",
    "\n",
    "    def read_raw_traffic(self):\n",
    "        \n",
    "        from pyspark.sql.types import StructType, StructField, StringType, IntegerType, FloatType, DoubleType, TimestampType\n",
    "        from pyspark.sql.functions import current_timestamp\n",
    "        print(\"Loading traffic data...\")\n",
    "        schema = StructType([\n",
    "            StructField(\"Record_ID\",IntegerType()),\n",
    "            StructField(\"Count_point_id\",IntegerType()),\n",
    "            StructField(\"Direction_of_travel\",StringType()),\n",
    "            StructField(\"Year\",IntegerType()),\n",
    "            StructField(\"Count_date\",StringType()),\n",
    "            StructField(\"hour\",IntegerType()),\n",
    "            StructField(\"Region_id\",IntegerType()),\n",
    "            StructField(\"Region_name\",StringType()),\n",
    "            StructField(\"Local_authority_name\",StringType()),\n",
    "            StructField(\"Road_name\",StringType()),\n",
    "            StructField(\"Road_Category_ID\",IntegerType()),\n",
    "            StructField(\"Start_junction_road_name\",StringType()),\n",
    "            StructField(\"End_junction_road_name\",StringType()),\n",
    "            StructField(\"Latitude\",DoubleType()),\n",
    "            StructField(\"Longitude\",DoubleType()),\n",
    "            StructField(\"Link_length_km\",DoubleType()),\n",
    "            StructField(\"Pedal_cycles\",IntegerType()),\n",
    "            StructField(\"Two_wheeled_motor_vehicles\",IntegerType()),\n",
    "            StructField(\"Cars_and_taxis\",IntegerType()),\n",
    "            StructField(\"Buses_and_coaches\",IntegerType()),\n",
    "            StructField(\"LGV_Type\",IntegerType()),\n",
    "            StructField(\"HGV_Type\",IntegerType()),\n",
    "            StructField(\"EV_Car\",IntegerType()),\n",
    "            StructField(\"EV_Bike\",IntegerType())\n",
    "            ])\n",
    "        \n",
    "        \n",
    "        df=(spark.readStream\n",
    "            .format(\"cloudFiles\")\n",
    "            .option(\"cloudFiles.format\", \"csv\")\n",
    "            .option(\"cloudFiles.schemaLocation\", f'{self.checkpoint_path}/raw_traffic/schemaInfer')\n",
    "            .schema(schema)\n",
    "            .load(f'{self.landing_path}/raw_traffic')\n",
    "            .withColumn('create_at', current_timestamp())\n",
    "         )\n",
    "        return df\n",
    "    \n",
    "    def read_raw_roads(self):\n",
    "        from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType\n",
    "        from pyspark.sql.functions import current_timestamp\n",
    "        schema = StructType([\n",
    "        StructField('Road_ID',IntegerType()),\n",
    "        StructField('Road_Category_Id',IntegerType()),\n",
    "        StructField('Road_Category',StringType()),\n",
    "        StructField('Region_ID',IntegerType()),\n",
    "        StructField('Region_Name',StringType()),\n",
    "        StructField('Total_Link_Length_Km',DoubleType()),\n",
    "        StructField('Total_Link_Length_Miles',DoubleType()),\n",
    "        StructField('All_Motor_Vehicles',DoubleType())\n",
    "        \n",
    "        ])\n",
    "\n",
    "        df=(spark.readStream\n",
    "        .format(\"cloudFiles\")\n",
    "        .option(\"cloudFiles.format\", \"csv\")\n",
    "        .option(\"cloudFiles.schemaLocation\", f'{self.checkpoint_path}/raw_roads/schemaInfer')\n",
    "        .schema(schema)\n",
    "        .load(f'{self.landing_path}/raw_roads')\n",
    "       \n",
    "        )\n",
    "        return df\n",
    "    \n",
    "    def write_raw_traffic(self,StreamingDF):\n",
    "        print(f'Writing data to {self.catalog} catalog raw_traffic table', end='' )\n",
    "        write_Stream = (StreamingDF.writeStream\n",
    "                    .format('delta')\n",
    "                    .option(\"checkpointLocation\",self.checkpoint_path + '/raw_traffic/Checkpt')\n",
    "                    .outputMode('append')\n",
    "                    .queryName('rawTrafficWriteStream')\n",
    "                    .trigger(availableNow=True)\n",
    "                    .toTable(f\"`{self.catalog}`.`bronze`.`raw_traffic`\"))\n",
    "    \n",
    "        write_Stream.awaitTermination()\n",
    "        print(f'Writing raw_traffic in {self.catalog} completed')\n",
    "      \n",
    "\n",
    "    def write_raw_roads(self,StreamingDF):\n",
    "        print(f'Writing data to {self.catalog} catalog raw_roads table', end='' )\n",
    "        write_Data = (StreamingDF.writeStream\n",
    "                        .format('delta')\n",
    "                        .option(\"checkpointLocation\",self.checkpoint_path + '/raw_roads/Checkpt')\n",
    "                        .outputMode('append')\n",
    "                        .queryName('rawRoadsWriteStream')\n",
    "                        .trigger(availableNow=True)\n",
    "                        .toTable(f\"`{self.catalog}`.`bronze`.`raw_roads`\"))\n",
    "        \n",
    "        write_Data.awaitTermination()\n",
    "        print(f'Writing raw_roads  in {self.catalog} completed')\n",
    "\n",
    "\n",
    "    def load_bronze_tables(self):\n",
    "        print('Loading bronze tables...', end='')\n",
    "        df_traffic = self.read_raw_traffic()\n",
    "        df_roads = self.read_raw_roads()\n",
    "        self.write_raw_traffic(df_traffic)\n",
    "        self.write_raw_roads(df_roads)\n",
    "        print('Loading bronze tables completed')\n",
    "\n",
    "    def assert_count(self,table_name,expected_count):\n",
    "        print(f'Asserting {table_name} count', end='')\n",
    "        actual_count = spark.read.table(f\"`{self.catalog}`.`bronze`.`{table_name}`\").count()\n",
    "        assert actual_count == expected_count, f'Expected {expected_count} records in {table_name}, found {actual_count} in {table_name}'\n",
    "        print(f'Asserting {table_name} count completed')\n",
    "\n",
    "    def validate(self):\n",
    "        import time\n",
    "        self.assert_count('raw_traffic',100000)\n",
    "        self.assert_count('raw_roads',100000)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b0104137-415f-486a-82b1-c13febd3ff78",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "GL= BronzeLoader(env)\n",
    "GL.load_bronze_tables()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "03-Bronze-loader",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
