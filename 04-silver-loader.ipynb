{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a4d16da4-74d1-4fa2-b6e9-15465f99c51d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Importing common settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "337fab92-46de-44de-b01d-65162dbbf04e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run /Workspace/Users/mbothe7@hotmail.com/databricks-end-to-end-project/01-initial-config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d9e5f140-9b4c-4769-9983-ce65cc433b94",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class SilverLoader():\n",
    "    def __init__(self, env):\n",
    "        Conf= InitialConfig()\n",
    "        self.catalog = env\n",
    "        self.silver_path=Conf.silver_path\n",
    "\n",
    "    def read_bronze_traffic(self):\n",
    "        print(f'Reading bronze table {self.catalog}.bronze.raw_traffic')\n",
    "        traffic = spark.readStream.table(f'{self.catalog}.bronze.raw_traffic')\n",
    "        print(f'Reading bronze table {self.catalog}.bronze.raw_traffic completed')\n",
    "        return traffic\n",
    "    \n",
    "    def read_bronze_roads(self):\n",
    "        print(f'Reading bronze table {self.catalog}.bronze.raw_roads')\n",
    "        roads = spark.readStream.table(f'{self.catalog}.bronze.raw_roads')\n",
    "        print(f'Reading bronze table {self.catalog}.bronze.raw_roads completed')\n",
    "        return roads\n",
    "    \n",
    "    def road_Category(self,df):\n",
    "        print('Creating Road Category Name Column: ', end='')\n",
    "        from pyspark.sql.functions import when,col\n",
    "\n",
    "        df_road = df.withColumn(\"Road_Category_Name\",\n",
    "                      when(col('Road_Category') == 'TA', 'Class A Trunk Road')\n",
    "                      .when(col('Road_Category') == 'TM', 'Class A Trunk Motor')\n",
    "                      .when(col('Road_Category') == 'PA','Class A Principal road')\n",
    "                       .when(col('Road_Category') == 'PM','Class A Principal Motorway')\n",
    "                       .when(col('Road_Category') == 'M','Class B road')\n",
    "                       .otherwise('NA')\n",
    "                     )\n",
    "        print(f'Creating Road Category Name Column completed')\n",
    "        return df_road\n",
    "    \n",
    "    def road_Type(self, df):\n",
    "        print('Creating Road Type Name Column: ', end='')\n",
    "        from pyspark.sql.functions import when,col\n",
    "\n",
    "        df_road_Type = df.withColumn(\"Road_Type\",\n",
    "                        when(col('Road_Category_Name').like('%Class A%'),'Major')\n",
    "                        .when(col('Road_Category_Name').like('%Class B%'),'Minor')\n",
    "                        .otherwise('NA')\n",
    "                        \n",
    "                        )\n",
    "        print('Creating Road Type Name Column completed')\n",
    "        return df_road_Type\n",
    "\n",
    "\n",
    "    def remove_duplicates(self, table_name):\n",
    "        print(f'Removing duplicates from {table_name}')\n",
    "        df=table_name.dropDuplicates()\n",
    "        print(f'Removing duplicates from {table_name} completed')\n",
    "        return df\n",
    "    \n",
    "    def remove_nulls(self, table_name,columns):\n",
    "        print(f'Removing nulls from {table_name}')\n",
    "        table_name=table_name.fillna('Unknown',subset=columns)\n",
    "\n",
    "        table_name=table_name.fillna(0,subset=columns)\n",
    "\n",
    "        return table_name\n",
    "    \n",
    "    def ev_count_add(self, df):\n",
    "        print(f'Adding EV count to the traffic table')\n",
    "        from pyspark.sql.functions import col\n",
    "        df_ev = df.withColumn('Electric_Vehicles_Count', col('EV_Car')+col('EV_Bike'))\n",
    "        print(f'Adding EV count to the traffic table   completed')\n",
    "        return df_ev\n",
    "    \n",
    "    \n",
    "    def motor_count_add(self, df):\n",
    "        print(f'Adding EV count to the traffic table ')\n",
    "        from pyspark.sql.functions import col\n",
    "        df_ev = df.withColumn('motor_count_add', col('Electric_Vehicles_Count') + col('Two_wheeled_motor_vehicles') + col('Cars_and_taxis') + col('Buses_and_coaches') + col('LGV_Type') + col('HGV_Type'))\n",
    "        print(f'Adding EV count to the traffic table  completed')\n",
    "        return df_ev\n",
    "    \n",
    "    def create_transformed_time(self,df):\n",
    "         from pyspark.sql.functions import current_timestamp\n",
    "         print('Creating Transformed Time column : ',end='')\n",
    "         df_timestamp = df.withColumn('Transformed_Time',current_timestamp() )\n",
    "         print('Creating Transformed Time column completed')\n",
    "         return df_timestamp\n",
    "    \n",
    "    def write_silver_traffic(self, df):\n",
    "        print(f'Writing silver table')\n",
    "\n",
    "        write_traffic = (df.writeStream\n",
    "                         .format('delta')\n",
    "                         .option(\"checkpointLocation\", f'{self.silver_path}/silver_traffic/checkpt/')\n",
    "                         .outputMode('append')\n",
    "                         .queryName(f'SilverTrafficStream')\n",
    "                         .trigger(availableNow=True)\n",
    "                         .toTable(f'{self.catalog}.silver.traffic_data'))\n",
    "        write_traffic.awaitTermination()\n",
    "        print(f'Writing silver table completed')\n",
    "\n",
    "    def write_silver_roads(self, df):\n",
    "        print(f'Writing silver roads table')\n",
    "\n",
    "        write_roads = (df.writeStream\n",
    "                         .format('delta')\n",
    "                         .option(\"checkpointLocation\", f'{self.silver_path}/silver_roads/checkpt/')\n",
    "                         .outputMode('append')\n",
    "                         .queryName(f'SilverRoadsStream')\n",
    "                         .trigger(availableNow=True)\n",
    "                         .toTable(f'{self.catalog}.silver.roads_data'))\n",
    "        write_roads.awaitTermination()\n",
    "        print(f'Writing silver roads table completed')\n",
    "\n",
    "    \n",
    "    \n",
    "    def load_silver_traffic(self, table_name='silver.traffic_data'):\n",
    "        print(f'Loading silver table {table_name}')\n",
    "        df=self.read_bronze_traffic()\n",
    "        df=self.remove_duplicates(df)\n",
    "        columns= df.schema.names\n",
    "        df=self.remove_nulls(df,columns)\n",
    "        df_clean=self.ev_count_add(df)\n",
    "        df_clean=self.motor_count_add(df_clean)\n",
    "        df_clean=self.create_transformed_time(df_clean)\n",
    "        self.write_silver_traffic(df_clean)\n",
    "        print(f'Loading silver table {table_name} completed')\n",
    "\n",
    "    def load_silver_roads(self, table_name='silver.roads_data'):\n",
    "        print(f'Loading silver table {table_name}')\n",
    "        df=self.read_bronze_roads()\n",
    "        df=self.remove_duplicates(df)\n",
    "        columns= df.schema.names\n",
    "        df=self.remove_nulls(df,columns)\n",
    "        df_clean=self.create_transformed_time(df)\n",
    "        df=self.road_Category(df)\n",
    "        df=self.road_Type(df)\n",
    "        self.write_silver_roads(df)\n",
    "        print(f'Loading silver table {table_name} completed')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "42ddfacb-0078-4523-bbdf-d57e5d4dc5b2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "m =SilverLoader(\"dev\")\n",
    "#traffic=m.load_silver_traffic()\n",
    "roads=m.load_silver_roads()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6c13fee2-148c-494b-a003-0bc55d6b2d28",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "04-silver-loader",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
